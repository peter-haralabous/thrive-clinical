# [Feature Name] - Discovery

> **Purpose**: Consolidate all research, competitive analysis, and user insights to validate that this problem exists and is worth solving before investing in a solution.

**Owner**: [PM Name]
**Status**: ğŸ” In Progress / âœ… Complete / âŒ Killed
**Research Period**: [Start Date] - [End Date]
**Last Updated**: [Date]

---

## Quick Links

- [User Research](#user-research-synthesis)
- [Market & Competitive Landscape](#market--competitive-landscape)
- [Data Analysis](#data--usage-analysis)
- [Business Impact Analysis](#business-impact-analysis)
- [Recommendation](#recommendation--next-steps)
- **Next Phase**: [Define Phase PRD](./initial-prd.md)

---

## Problem Hypothesis

**We believe that** [user persona] experiences [problem/pain point]

**This is important because** [impact on users and business]

**If we solve it**, users will [outcome], which will lead to [metric improvement]

**Strategic alignment**: [How this connects to product-strategy.md goals]

---

## Discovery Sources

*All research inputs that informed this discovery*

### User Research
- [ ] User interviews: [N participants] - [Link to Google Drive folder]
- [ ] Survey: [N responses] - [Link to results]
- [ ] Usability testing: [N sessions] - [Link to recordings]
- [ ] Customer support tickets: [N analyzed] - [Link to analysis]

### Data Sources
- [ ] Usage analytics: [Link to dashboard]
- [ ] Cohort analysis: [Link to analysis]
- [ ] A/B test results: [Link to results]
- [ ] Historical metrics: [Timeframe analyzed]

### Market Research
- [ ] Competitive analysis: [N competitors reviewed]
- [ ] Industry reports: [Sources]
- [ ] Expert interviews: [N experts]
- [ ] Market trends: [Sources]

### Internal Sources
- [ ] Sales/CS feedback: [Link to synthesis]
- [ ] Stakeholder interviews: [Who we talked to]
- [ ] Internal dogfooding: [Team feedback]
- [ ] Technical feasibility: [Link to spike if conducted]

---

## User Research Synthesis

*Consolidated findings from all user research activities*

### Research Approach

**Methods used**: [Interviews / Surveys / Observation / Testing]
**Participants**: [N total across all methods]
- [Segment 1]: [N participants]
- [Segment 2]: [N participants]

**Key research questions**:
1. Does this problem exist?
2. How severe is it?
3. How often do users encounter it?
4. What do users currently do to cope?
5. Would they value a solution?

### Theme 1: [Major Pain Point]

**Prevalence**: [N/M users mentioned this] ([X%])
**Severity**: ğŸ”´ Critical / ğŸŸ¡ Moderate / ğŸŸ¢ Minor

**Evidence**:
- User interviews: [Summary of what we heard]
- Survey data: [Relevant stats]
- Support tickets: [Pattern observed]

**Representative quotes**:
> "[Quote that captures the pain]" - [User role/segment]
> "[Another compelling quote]" - [User role/segment]

**Observed behaviors**:
- [Behavior 1 we saw]
- [Behavior 2 we saw]

**Current workarounds**:
- [How users solve this today]
- [Why these workarounds fall short]

**Impact on users**:
- [Time wasted / Money lost / Frustration / Risk]

---

### Theme 2: [Another Major Finding]

**Prevalence**: [N/M users] ([X%])
**Severity**: ğŸ”´ Critical / ğŸŸ¡ Moderate / ğŸŸ¢ Minor

[Same structure as Theme 1]

---

### Theme 3: [Additional Finding]

[Same structure]

---

### User Segmentation Insights

**Segment A: [Name]** ([X%] of users)
- Characteristics: [How to identify them]
- Pain points: [What they experience most]
- Needs: [What they value most]
- Quote: "[Representative quote]"

**Segment B: [Name]** ([X%] of users)
[Same structure]

**Recommended target segment**: [Which segment to focus on and why]

---

### Jobs-to-be-Done Analysis

**When** [situation/context]

**Users want to** [job/goal]

**So they can** [desired outcome]

**But currently** [obstacle/frustration]

**Alternative jobs competing for attention**:
- [Alternative approach 1]
- [Alternative approach 2]

**Why current solutions fail**:
- [Reason 1]
- [Reason 2]

---

## Data & Usage Analysis

*Quantitative evidence from product analytics and metrics*

### Current State Metrics

**Baseline measurements**:
- [Metric 1]: [Current value]
- [Metric 2]: [Current value]
- [Metric 3]: [Current value]

**Trend analysis**:
- [Metric] has [increased/decreased] by [X%] over [timeframe]
- [Pattern observed in data]

### Behavioral Patterns

**Usage data reveals**:
- [Pattern 1: e.g., "Users abandon flow at step 3 - 45% drop-off"]
- [Pattern 2: e.g., "Feature X used by 20% of users but accounts for 60% of support tickets"]
- [Pattern 3: e.g., "Power users create 3x workarounds vs casual users"]

**Correlation analysis**:
- Users who experience [problem] are [X%] more likely to [negative outcome]
- [Metric A] correlates with [Metric B] (r=[correlation coefficient])

### Cohort Insights

**New users** ([Timeframe]):
- [Observation about how new users experience this]

**Power users** ([Definition]):
- [Observation about how power users experience this]

**Churned users** ([Definition]):
- [Pattern in why users churned related to this problem]

---

## Market & Competitive Landscape

*External context: what exists in the market and where gaps are*

### Market Context

**Market size**: [TAM/SAM if relevant]

**Market trends**:
- [Trend 1: e.g., "Increasing demand for X"]
- [Trend 2: e.g., "Regulatory pressure around Y"]
- [Trend 3: e.g., "Technology enabler Z now mature"]

**Timing factors**:
- [Why now is the right time]
- [What's changed in the market]

### Competitive Analysis

#### Competitor A: [Name]

**Their approach**: [How they solve this problem]

**Strengths**:
- [What they do well]
- [Why users choose them]

**Weaknesses**:
- [What they don't solve]
- [User complaints we found]

**Pricing**: [Their pricing model if relevant]

**Market position**: [Market share / User base]

---

#### Competitor B: [Name]

[Same structure]

---

#### Competitor C: [Name]

[Same structure]

---

### Competitive Gaps & Opportunities

**What competitors get right**:
- [Industry best practice 1]
- [Industry best practice 2]

**Where competitors fall short**:
- [Gap 1: Problem they don't solve]
- [Gap 2: User need they ignore]
- [Gap 3: Approach that doesn't work]

**Our differentiation opportunity**:
- [How we could approach this differently]
- [Unique advantage we have]

**White space**: [Unmet need in market]

---

## Business Impact Analysis

*Comprehensive assessment of whether this problem is worth solving*

### Market Opportunity

**Target users in our product**:
- [N current users] directly affected by this problem
- [Segment breakdown]: [X% segment A], [Y% segment B]
- [Growth projection]: Expected to reach [N users] in [timeframe]

**Addressable market**:
- TAM (Total Addressable Market): [N potential users or $X]
- SAM (Serviceable Addressable Market): [N potential users or $X]
- Our target: [N users or $X in timeframe]

**Competitive positioning**:
- This positions us as: [Market position if we solve this]
- Competitive advantage: [How this differentiates us]
- Market gap we'd fill: [Opportunity in market]

### Cost of the Problem

**Current user impact**:
- Time wasted: [X hours/week per user] Ã— [N users] = [Total hours/week]
- Error rate: [X% mistakes] leading to [consequence]
- Workaround effort: [Description of current coping mechanisms]
- Friction points: [Specific pain in user journey]

**Current business impact**:
- Support costs: [$X per month] handling [N related tickets]
- Churn impact: [X% of churned users] cited this problem
- Lost revenue: Estimated [$X annually] in lost conversions/upsells
- Competitive vulnerability: [Risk of losing users to competitors]
- Brand impact: [NPS/CSAT impact if quantifiable]

**Total annual cost of problem**: [$X] (or [X hours], or [other quantification])

### Value of Solving

**User value if we solve this**:
- Time saved: [X hours/week per user]
- Success rate improvement: [From X% to Y%]
- Reduced friction: [Specific improvement in user experience]
- User satisfaction: Expected [+X points] in NPS/CSAT

**Business value if we solve this**:
- Revenue opportunity: [$X annually] from:
  - Improved conversion: [+X%]
  - Reduced churn: [-X%]
  - Increased usage: [+X%]
  - New user acquisition: [Estimated growth]
- Cost savings: [$X annually] from:
  - Reduced support volume: [-X tickets/month]
  - Operational efficiency: [Specific savings]
- Strategic value:
  - Competitive advantage: [How this positions us]
  - Platform strength: [How this enables future features]
  - Market expansion: [New segments we can address]

**Value calculation**:
```
Example calculation showing your work:

Users affected: 1,000
Current completion rate: 40%
Expected completion rate with solution: 60%
Additional conversions: 1,000 Ã— (60% - 40%) = 200

Value per conversion: $50
Additional annual revenue: 200 Ã— $50 Ã— 12 = $120,000

Development cost estimate: $30,000
Payback period: 3 months
3-year ROI: 1,440%
```

### Success Metrics Framework

**North Star Metric**: [Primary metric this impacts]

**Primary success metric**:
- **Metric name**: [e.g., "Afternoon medication adherence rate"]
- **Current baseline**: [40%]
- **Target**: [60%] within [90 days]
- **How we'll measure**: [Specific tracking method]
- **Dashboard**: [Link to analytics dashboard]

**Secondary success metrics**:

1. **[Metric 2 name]**: [e.g., "User satisfaction with reminders"]
   - Baseline: [Current value]
   - Target: [Goal]
   - Measurement: [How we'll track]

2. **[Metric 3 name]**: [e.g., "Support ticket volume"]
   - Baseline: [Current value]
   - Target: [Goal]
   - Measurement: [How we'll track]

**Leading indicators** (early signals of success):
- [Indicator 1]: [What this measures and why it predicts success]
- [Indicator 2]: [What this measures and why it predicts success]

**Counter-metrics** (watch for negative impacts):
- [Metric to monitor]: [What we're concerned about]
- [Threshold]: [When to investigate or take action]

### Measurement Plan

**Analytics instrumentation needed**:
- [ ] [Event 1 to track]: [When it fires]
- [ ] [Event 2 to track]: [When it fires]
- [ ] [Event 3 to track]: [When it fires]

**Data collection approach**:
- Quantitative: [Analytics events, system logs, etc.]
- Qualitative: [Surveys, interviews, feedback channels]

**Review cadence**:
- **Week 1-2**: Daily review (catch critical issues early)
- **Week 3-4**: Every 2-3 days
- **Month 2**: Weekly review
- **Month 3+**: Bi-weekly or monthly

**Success timeline**:
- **30 days**: [Expected early results]
- **60 days**: [Expected intermediate results]
- **90 days**: [Expected full results / decision point]

---

## Impact Forecast

*Predicted impact on key metrics with confidence levels*

### Primary Metric Impact

**Metric**: [North star or primary metric name]

**Current baseline**: [Current value]

**Predicted impact**: [+X%] improvement

**Target value**: [Specific goal]

**Confidence level**: ğŸŸ¢ High (>80%) / ğŸŸ¡ Medium (50-80%) / ğŸ”´ Low (<50%)

**Impact model**:
```
[Show your detailed reasoning]

Example:
Assumption 1: 1,000 users affected
Assumption 2: Current behavior rate = 40%
Assumption 3: Solution will improve rate to 60% (based on prototype testing)

Impact calculation:
- Current: 1,000 Ã— 40% = 400 successful actions
- With solution: 1,000 Ã— 60% = 600 successful actions
- Improvement: +200 actions (+50% relative improvement)

Supporting evidence:
- Similar feature at Competitor X showed 45% improvement
- Our prototype testing showed 55% improvement (n=20)
- Conservative estimate: 50% improvement
```

**Sensitivity analysis**:
- Best case: [+X%] if [optimistic assumption]
- Expected case: [+X%] if [realistic assumption]
- Worst case: [+X%] if [conservative assumption]

---

### Secondary Metric Impacts

**[Metric 2]**: [Name]
- Current: [Value]
- Predicted: [+X%]
- Confidence: ğŸŸ¢/ğŸŸ¡/ğŸ”´
- Rationale: [Brief explanation]

**[Metric 3]**: [Name]
- Current: [Value]
- Predicted: [+X%]
- Confidence: ğŸŸ¢/ğŸŸ¡/ğŸ”´
- Rationale: [Brief explanation]

---

### Financial Impact Summary

**Revenue impact** (if applicable):
- Year 1: [$X] from [source]
- Year 2: [$Y] (assuming [growth rate])
- 3-year total: [$Z]

**Cost savings** (if applicable):
- Annual savings: [$X] from [source]

**Development investment**:
- Estimated development cost: [$X]
- Estimated ongoing cost: [$Y per month/year]

**Return on Investment**:
- Payback period: [X months]
- 1-year ROI: [X%]
- 3-year ROI: [X%]

---

## Risk Assessment & Assumptions

### Critical Assumptions

*Assumptions that, if wrong, would significantly impact success*

| # | Assumption | Confidence | Impact if Wrong | How to Validate | Status |
|---|------------|------------|-----------------|-----------------|--------|
| 1 | Users will change their workflow | ğŸŸ¡ Medium | ğŸ”´ High | Prototype testing with real users | [ ] |
| 2 | Problem occurs frequently enough to matter | ğŸŸ¢ High | ğŸ”´ High | Usage data analysis | [âœ“] |
| 3 | We can build this in [timeframe] | ğŸŸ¡ Medium | ğŸŸ¡ Medium | Technical spike | [ ] |
| 4 | Solution won't cannibalize feature X | ğŸŸ¢ High | ğŸŸ¡ Medium | User interviews + analytics | [ ] |
| 5 | Target metric improvement of X% is achievable | ğŸŸ¡ Medium | ğŸ”´ High | Prototype results + competitor benchmarks | [ ] |
| 6 | Users will pay/adopt at expected rate | ğŸŸ¡ Medium | ğŸ”´ High | Pricing research / Beta testing | [ ] |

**Validation plan**:
- [ ] [Assumption 1]: Validate via [method] by [date]
- [ ] [Assumption 2]: Validate via [method] by [date]

---

### Risks & Mitigation Strategies

**Risk 1: [e.g., "Low adoption despite solving real problem"]**
- **Likelihood**: ğŸ”´ High / ğŸŸ¡ Medium / ğŸŸ¢ Low
- **Impact if occurs**: ğŸ”´ High / ğŸŸ¡ Medium / ğŸŸ¢ Low
- **Evidence this is a risk**: [Why we think this could happen]
- **Mitigation strategy**: [What we'll do to reduce likelihood]
- **Contingency plan**: [What we'll do if it happens]

**Risk 2: [e.g., "Technical complexity exceeds estimates"]**
- **Likelihood**: ğŸŸ¡ Medium
- **Impact if occurs**: ğŸŸ¡ Medium
- **Evidence**: [Past experience, technical unknowns]
- **Mitigation**: [Technical spike, expert consultation]
- **Contingency**: [Descope or timeline adjustment]

**Risk 3: [e.g., "Competitive response"]**
- **Likelihood**: ğŸŸ¡ Medium
- **Impact if occurs**: ğŸŸ¡ Medium
- **Evidence**: [Competitor behavior patterns]
- **Mitigation**: [Speed to market, differentiation]
- **Contingency**: [Pivot strategy or double down]

**Risk 4: [e.g., "Solution doesn't move primary metric"]**
- **Likelihood**: ğŸŸ¡ Medium
- **Impact if occurs**: ğŸ”´ High
- **Evidence**: [Uncertainty in impact model]
- **Mitigation**: [Phased rollout, early measurement]
- **Contingency**: [Iteration plan or kill criteria]

---

### Open Questions

*Things we still need to learn before committing fully*

- [ ] **Question 1**: [What we still don't know]
  - **Why it matters**: [Impact on decision/approach]
  - **How we'll answer**: [Research method or experiment]
  - **By when**: [Timeline for getting answer]
  - **Blocking**: Yes / No

- [ ] **Question 2**: [What we still don't know]
  - **Why it matters**: [Impact]
  - **How we'll answer**: [Method]
  - **By when**: [Timeline]
  - **Blocking**: Yes / No

---

## Go/No-Go Framework

### Criteria for Proceeding to Define Phase

**Must-haves** (all must be true):
- âœ…/âŒ Problem validated by [N+] users in target segment
- âœ…/âŒ Data shows problem impacts [X%+] of users or costs [$Y+]
- âœ…/âŒ Predicted ROI > [threshold, e.g., 3x development cost]
- âœ…/âŒ Strategic alignment with [product-strategy.md goals]
- âœ…/âŒ No fatal technical blockers (confirmed via spike)
- âœ…/âŒ Success metrics are measurable and trackable

**Should-haves** (2 of 3 recommended):
- â­• Clear competitive differentiation opportunity
- â­• High confidence (ğŸŸ¢) in primary metric impact
- â­• User willingness to pay/adopt validated

**Deal-breakers** (any one kills the project):
- âŒ Problem doesn't exist or is too rare
- âŒ ROI doesn't justify investment
- âŒ Technically infeasible or too risky
- âŒ Better alternatives available (competitors solved it well)
- âŒ Wrong strategic timing

---

### Current Status Assessment

**Must-haves status**: [X/6] met

**Should-haves status**: [X/3] met

**Deal-breakers present**: Yes / No - [If yes, which one]

**Overall recommendation readiness**: ğŸŸ¢ Ready / ğŸŸ¡ Needs more work / ğŸ”´ Not ready

---

## Solution Hypotheses

*Early thinking on potential approaches (not final solutions)*

### Hypothesis 1: [Solution Approach Name]

**Description**: [What this solution would do]

**How it addresses the problem**: [Connection to findings]

**Pros**:
- [Benefit 1]
- [Benefit 2]
- [Benefit 3]

**Cons**:
- [Drawback 1]
- [Drawback 2]

**Estimated effort**: ğŸŸ¢ Small (1-2 weeks) / ğŸŸ¡ Medium (1-2 months) / ğŸ”´ Large (3+ months)

**Technical feasibility**: ğŸŸ¢ Straightforward / ğŸŸ¡ Moderate complexity / ğŸ”´ High risk

**Evidence supporting this approach**:
- [Finding from research that supports this]
- [Competitor example or market validation]
- [User feedback suggesting this direction]

**Expected impact on primary metric**: [Rough estimate]

---

### Hypothesis 2: [Alternative Approach]

[Same structure]

---

### Hypothesis 3: [Alternative Approach]

[Same structure]

---

### Recommended Starting Point

**Approach to explore first**: [Which hypothesis and why]

**Rationale**:
- [Reason 1 based on research findings]
- [Reason 2 based on effort/value balance]
- [Reason 3 based on risk/feasibility]

**Key assumptions to validate in Define phase**:
- [ ] [Assumption 1 that needs testing with prototypes]
- [ ] [Assumption 2 that needs validation]
- [ ] [Assumption 3 that needs proof]

**Alternative approaches to keep warm**:
- [Backup hypothesis if primary doesn't work]

---

## Recommendation & Next Steps

### Decision

**Recommendation**: âœ… Proceed to Define / ğŸ”„ Need More Research / âŒ Kill

**Confidence in recommendation**: ğŸŸ¢ High / ğŸŸ¡ Medium / ğŸ”´ Low

**Decision rationale**:

**Why proceed** (if âœ…):
- âœ… Problem is real: [Evidence from research]
- âœ… Impact is significant: [Business case summary]
- âœ… Solution is feasible: [Technical/user validation]
- âœ… Strategic fit: [Alignment with goals]
- âœ… Go/no-go criteria: [X/X must-haves met]

**What gives us confidence**:
- [Strong signal 1 from research]
- [Strong signal 2 from data]
- [Strong signal 3 from market]

**What makes us cautious**:
- [Concern 1 to watch in Define phase]
- [Concern 2 to monitor]
- [Risk 3 to mitigate]

---

**Why more research needed** (if ğŸ”„):
- [Gap 1 in understanding]
- [Gap 2 in validation]
- [Open question blocking decision]

**Research plan**:
- [ ] [Specific research to conduct]
- [ ] [Timeline for completion]
- [ ] [Decision point after research]

---

**Why killing** (if âŒ):
- âŒ [Deal-breaker that emerged]
- âŒ [Why this isn't worth solving]
- âŒ [Better opportunity elsewhere]

**What we learned**:
- [Insight to carry forward]
- [Where to redirect resources]

---

### If Proceeding to Define Phase

**Immediate next steps**:
1. [ ] Create initial-prd.md with validated problem statement
2. [ ] Schedule stakeholder review for [date]
3. [ ] Plan solution exploration (prototyping approach)
4. [ ] Set up analytics tracking for baseline metrics
5. [ ] Begin technical feasibility spike if needed

**Phase owner**: [Who will own Define phase]

**Timeline**:
- Define phase start: [Date]
- Expected completion: [Date]
- Target decision point: [Date]

**Success criteria for Define phase**:
- [ ] Problem statement validated with stakeholders
- [ ] Solution hypotheses tested with prototypes
- [ ] Technical feasibility confirmed
- [ ] Go/no-go decision made with leadership

**Investment approved**:
- Discovery investment: [$X] (completed)
- Define phase budget: [$X] (approved/pending)
- Total project budget: [$X] (if approved for build)

---

## Appendix: Source Materials

### Research Documents
- [Google Drive: User interview transcripts folder]
- [Google Drive: Survey results spreadsheet]
- [Google Drive: Usability test recordings]
- [Google Drive: Customer support ticket analysis]
- [Figma: Early concept sketches if any]

### Data & Analysis
- [Analytics dashboard: Usage metrics]
- [Spreadsheet: Cohort analysis]
- [Spreadsheet: Financial impact model]
- [Document: Support ticket analysis]

### Market Research
- [Industry report: Market trends]
- [Document: Competitive analysis detailed]
- [Document: Expert interview summaries]
- [Slides: Market sizing analysis]

### Internal References
- [Slack thread: Initial discussion]
- [Meeting notes: Stakeholder input sessions]
- [Document: Technical spike results]
- [Related feature: Prior art in our product]
- [product-strategy.md: Strategic context]

---

## Document History

| Date | Change | Author |
|------|--------|--------|
| [Date] | Created discovery doc | [Name] |
| [Date] | Added user research findings | [Name] |
| [Date] | Added competitive analysis | [Name] |
| [Date] | Added business impact analysis | [Name] |
| [Date] | Final recommendation & decision | [Name] |
